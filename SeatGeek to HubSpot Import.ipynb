{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seat Geek to Hubspot ETL\n",
    "\n",
    "This code describes the process to transfer ticket transcation data from Seat Geek export to Hubspot. This data supports ROI tracking on marketing assets and customer segmentation for sales/marketing strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Extract data from Unify\n",
    "\n",
    "Depending on the upload, if this is to be done after each game you will head to Unify, go to reports, select Seats Sold (1 per row) and download the file. For the upload I am doing currently we are backfilling all of 2024 so we will be using all events in the report. But otherwise, select the date range that your event took place and then click the events drop down and select your event. Download this file as a csv.\n",
    "\n",
    "This file contains every seat for that game including seat, customer, and price information.\n",
    "\n",
    "We will use this data in order to build out our individual game transactions in HubSpot as well as import the customers that were created\n",
    "\n",
    "This data will come in the raw export form, no adjustments are needed before running this code, simply update the csv you are reading and run. All columns will be cleaned to fit the correct names and those that are unnecessary will be dropped\n",
    "\n",
    "At the end of this step you will be left with two dataframes, one for tickets and one for customers. The one for tickets will be used to create deals and line items to be imported into HubSpot. The customers will be cleaned and assigned the properties needed them imported into HubSpot as well. <b> Customers must be imported into HubSpot before Deals </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tickets = pd.read_csv('Seats Sold (1 Seat Per Row) 2024-11-13 11_45_40.csv', low_memory=False)\n",
    "\n",
    "Season = 2024\n",
    "\n",
    "# Update column names step by step\n",
    "tickets.columns = tickets.columns.str.replace(\"MainTable_DetailsValue\", \"\", regex=False)\n",
    "tickets.columns = tickets.columns.str.replace(\"TikOwner\", \"\", regex=False)\n",
    "tickets.columns = tickets.columns.str.replace(\"1\", \"\", regex=False)\n",
    "tickets.columns = tickets.columns.str.replace(\"AddressLine2_8\", \"AddressLineX\", regex=False)\n",
    "tickets.columns = tickets.columns.str.replace(\"[023456789_]\", \"\", regex=True)\n",
    "tickets.columns = tickets.columns.str.replace(\"AddressLineX\", \"AddressLine2\", regex=False)\n",
    "tickets.columns = tickets.columns.str.replace(\"Descr\", \"\", regex=False)\n",
    "\n",
    "# Columns to keep\n",
    "columns_to_keep = [\n",
    "    \"EventDate\", \"EventName\", \"FirstName\", \"LastName\", \"AddressLine\", \"AddressLine2\", \n",
    "    \"City\", \"State\", \"ZipCode\", \"Country\", \"Email\", \"HomePhone\", \n",
    "    \"BusinessPhone\", \"MobilePhone\", \"Area\", \"Sector\", \"Row\", \"Seat\", \"SeatType\", \n",
    "    \"PriceLevel\", \"PriceTypeGroup\", \"PriceType\", \"TransactNum\", \"TransactDate\", \n",
    "    \"TotalSaleValue\", \"IsScanned\"\n",
    "]\n",
    "\n",
    "# Perform column selection\n",
    "tickets = tickets[columns_to_keep]\n",
    "\n",
    "# Creating the 'customers' dataframe with the specified columns\n",
    "customers_columns = [\n",
    "    \"FirstName\", \"LastName\", \"AddressLine\", \"AddressLine2\", \"City\", \"State\", \n",
    "    \"ZipCode\", \"Country\", \"Email\", \"HomePhone\", \"BusinessPhone\", \"MobilePhone\", \"PriceTypeGroup\", \"PriceType\"\n",
    "]\n",
    "\n",
    "# Create the 'customers' dataframe\n",
    "customers = tickets[customers_columns]\n",
    "\n",
    "# Drop the specified columns from tickets except 'Email'\n",
    "columns_to_drop = [\n",
    "    \"FirstName\", \"LastName\", \"AddressLine\", \"AddressLine2\", \"City\", \"State\", \"SeatType\",\n",
    "    \"ZipCode\", \"Country\", \"HomePhone\", \"BusinessPhone\", \"MobilePhone\", \"Area\", \"Row\", \"Seat\"\n",
    "]\n",
    "\n",
    "# Update the 'tickets' dataframe\n",
    "tickets = tickets.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Import New Contacts to Hubspot\n",
    "\n",
    "We must first migrate our unique contacts to HubSpot to ensure we don't have errors upon importing deals, there are two types of errors we must account for:\n",
    "\n",
    "1. <b> Invalid Email </b> -- This we solve above when correcting the email domains, HubSpot will only import valid email addresses\n",
    "2. <b> Invalid Duplicate ID </b> -- This error occurs when a contact is created in an import then appears again in the import. Since we are importing multiple deals associating to contacts we are at risk of this error. This only occurs when a contact is created in the import so by first importing the contacts then the deals we ensure that we won't create any contacts when importing deals removing this risk\n",
    "\n",
    "When importing these contacts we want to ensure we are including all the data exported from Unify into HubSpot. We also want to ensure this data is clean and that contacts are being put in the correct business unit. The exported file from Step 1 has the correct columns needed. This code below will clean and match them to get us an export of unique emails with cleaned information\n",
    "\n",
    "1. <b> Find Business units </b> -- Add the LUFC business unit - this is necessary for import into HubSpot\n",
    "2. <b> Add Contact owner </b> -- Add the contact owners\n",
    "3. <b> Clean customer emails </b> -- Clean emails, ensure no empty ones\n",
    "4. <b> Clean customer columns </b> -- We then want to clean the customers data that we will be importing into HubSpot. This includes formating name, address, and phone number data as well as removing the columns no longer needed\n",
    "5. <b> THERE WILL BE MORE STEPS IN THE FUTURE </b> -- For this offseason import it is not import but when moving towards imports after each game when the season is going this code will need to be adjusted to have parts after. What these entail is you need to import all buyers which is what this will create, but then you also want to import the single game buyers and break them down by phone or no phone. To do this you would import the customers file that's outputted. Then export all contacts from that import including their \"Phone Number\" column and the \"Contact Owner - LUFC\". These would then get matched back to the original import based on Email. You would merged the Phone Number column and then filter on PriceTypeGroup is Single. This would then be split into those with phone numbers and those without and each of these would be imported and if a contact had contact ownership of the info account, that would be reassigned to one of their account executives. Each of these new imports, one with phone numbers one without, would be imported <b> with overwriting on the \"Contact Owner - LUFC\" </b>\n",
    "\n",
    "After these steps are followed you are left with a file to import into HubSpot which will ensure all of our customers contacts are in the system before importing the deals which will allow them to match. When importing, match each column to its corresponding HubSpot column, all of these are self explanatory when matching. It is important that you <b> do not overwrite </b> is selected for all columns\n",
    "\n",
    "##### <b> Do not overwrite columns on contact import </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Drop duplicate emails\n",
    "customers = customers.drop_duplicates(subset='Email')\n",
    "\n",
    "# Function to filter out malformed emails (doesn't contain @ symbol)\n",
    "def format_emails(email):\n",
    "    if pd.notna(email) and '@' in email:\n",
    "        return email\n",
    "    return ''\n",
    "\n",
    "# Function to fix invalid emails\n",
    "def format_invalid_emails(email):\n",
    "    com_variations = [',com', '.cim', '.ocm', '.C0M', '.comm', '. COM', '.coom', '.cop', '.oom', '.lcom', '.con', '.co,m', '..com', '.cpm', '.cpom', '.col', '.co,', '.xom', '.fom', '.coke', 'cok', '.come', '..com', 'clom', '.com.']\n",
    "    net_variations = [',net', '.met', '.ent', '.nat', '.nt', '.ner', '.ney', '.lnet', '. net', '.nety', '.neto', '.ncet', '/net', '.nedt', '.nett', '.nbet']\n",
    "    \n",
    "    # Skip processing if the email is NaN or empty\n",
    "    if pd.isna(email) or email == '':\n",
    "        return email\n",
    "    \n",
    "    # Remove spaces from the email\n",
    "    email = email.replace(\" \", \"\")\n",
    "    \n",
    "    # Fix .com variations\n",
    "    for variation in com_variations:\n",
    "        if variation in email:\n",
    "            email = email.replace(variation, \".com\")\n",
    "            break\n",
    "    \n",
    "    # Fix .net variations\n",
    "    for variation in net_variations:\n",
    "        if variation in email:\n",
    "            email = email.replace(variation, \".net\")\n",
    "            break\n",
    "\n",
    "    # Remove consecutive dots\n",
    "    while '..' in email:\n",
    "        email = email.replace('..', '.')\n",
    "    \n",
    "    return email\n",
    "\n",
    "\n",
    "# Function to format phone numbers\n",
    "def format_phone_number(phone):\n",
    "    if pd.isna(phone) or isinstance(phone, str) and not re.search(r'\\d', phone):\n",
    "        return None\n",
    "    else:\n",
    "        clean_phone = re.sub(r'[^\\d]', '', str(phone))  # Remove all non-digit characters\n",
    "        if re.match(r'(\\d{10})$', clean_phone):\n",
    "            # category 1, complete number\n",
    "            formatted_number = f'+1{clean_phone}'\n",
    "            return formatted_number\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Function to format zip codes\n",
    "def format_zip_codes(zip_code):\n",
    "    if pd.isna(zip_code) or not isinstance(zip_code, str):\n",
    "        return ''\n",
    "    zip_code = re.sub(r'\\D', '', zip_code)  # Remove non-digit characters\n",
    "    if len(zip_code) >= 5:\n",
    "        return zip_code[:5]  # Return the first 5 characters\n",
    "    return ''\n",
    "\n",
    "# Create the 'Business Unit' column\n",
    "customers['Business Unit'] = ';Loudoun United FC'\n",
    "customers['Contact Owner - LUFC'] = 'info@loudoununitedfc.com'\n",
    "\n",
    "# Apply the functions to clean 'Email', skipping NaN values\n",
    "customers['Email'] = customers['Email'].apply(lambda x: format_emails(x) if pd.notna(x) else x)\n",
    "customers['Email'] = customers['Email'].apply(lambda x: format_invalid_emails(x) if pd.notna(x) else x)\n",
    "\n",
    "# Drop rows with no email\n",
    "customers = customers[customers['Email'].notna() & (customers['Email'] != '')]\n",
    "\n",
    "# Set country to US (comes as USA)\n",
    "customers['Country'] = 'US'\n",
    "\n",
    "# Ensure columns are in title case\n",
    "customers['FirstName'] = customers['FirstName'].str.title()\n",
    "customers['LastName'] = customers['LastName'].str.title()\n",
    "customers['AddressLine'] = customers['AddressLine'].str.title()\n",
    "customers['AddressLine2'] = customers['AddressLine2'].str.title()\n",
    "customers['City'] = customers['City'].str.title()\n",
    "\n",
    "# Apply the function to the 'Zip' column to clean\n",
    "customers['ZipCode'] = customers['ZipCode'].apply(format_zip_codes)\n",
    "\n",
    "# Apply the function to the 'Phone' column to clean\n",
    "customers['HomePhone'] = customers['HomePhone'].apply(format_phone_number)\n",
    "customers['BusinessPhone'] = customers['BusinessPhone'].apply(format_phone_number)\n",
    "customers['MobilePhone'] = customers['MobilePhone'].apply(format_phone_number)\n",
    "\n",
    "# Creating the Phone Number and filling it with HomePhone, or MobilePhone, or BusinessPhone\n",
    "customers['Phone Number'] = customers['HomePhone'].fillna(customers['MobilePhone']).fillna(customers['BusinessPhone'])\n",
    "\n",
    "# Drop the 'HomePhone' column\n",
    "customers = customers.drop(columns=['HomePhone'])\n",
    "\n",
    "# Drop duplicate emails\n",
    "customers = customers.drop_duplicates(subset='Email')\n",
    "\n",
    "# Save file for Import\n",
    "customers.to_csv('Import Customers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Deal Data Transformation\n",
    "\n",
    "We perform several data transformations to improve data quality and make the data acceptable for import to Hubspot\n",
    "\n",
    "1. <b> Email domain corrections </b> -- Replace invalid domain exentions like \".comm\" with \".com\" or their likely equaivalent\n",
    "2. <b> Convert tickets scanned to 1,0 </b> -- Reconfigure [IsScanned] to binary to sum when grouping to get the total number of tickets scanned per transaction\n",
    "3. <b> Set ticket price to numeric </b> -- Originally had issues with some prices being in different formats, this ensures there are no issues when calculating total price\n",
    "4. <b> Set price for certain ticket types to 0 </b> -- Certain deals are already tracked in HubSpot, we set these types to zero as to not double count our revenues\n",
    "5. <b> Adjusting close date </b> -- Some transactions have tickets without close dates, these are populated with the same date as the rest of the transaction or the event date as when importing to HubSpot if there is no closed date on a closed won deal it will default to the create date and we don't want deals to have close dates after the events for individual tickets\n",
    "6. <b> Remove tickets without emails </b> -- Our purpose for this import is to calculate ROI and support sales. There are few deals without emails so it is not impactful on revenue reporting. This is important to remove as otherwise it will cause issues in our imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the part of EventName before \" at \" and then rename to Opponent\n",
    "tickets['EventName'] = tickets['EventName'].str.split(\" at \").str[0]\n",
    "\n",
    "tickets = tickets.rename(columns={\n",
    "    \"EventName\": \"Opponent\",\n",
    "    \"TransactDate\": \"Close Date\",\n",
    "    \"Sector\": \"Section\",\n",
    "    \"IsScanned\": \"IsScanned\",\n",
    "    \"TotalSaleValue\": \"Amount\",\n",
    "    \"PriceTypeGroup\": \"Individual Deal Type\"\n",
    "})\n",
    "\n",
    "# Ensure 'Amount' is a numeric float\n",
    "tickets['Amount'] = pd.to_numeric(tickets['Amount'], errors='coerce').astype(float)\n",
    "\n",
    "# Adjust the Amount based on Individual Deal Type\n",
    "tickets.loc[tickets['Individual Deal Type'] != 'Single', 'Amount'] = 0\n",
    "\n",
    "# Set any remaining blank Amount values to 0\n",
    "tickets['Amount'] = tickets['Amount'].fillna(0)\n",
    "\n",
    "# Change 'Yes' to 1 and 'No' to 0 in the \"IsScanned\" column\n",
    "tickets['IsScanned'] = tickets['IsScanned'].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "# Create a new column 'line_item_name' as PriceLevel + PriceType\n",
    "tickets['line_item_name'] = tickets['PriceLevel'] + \" \" + tickets['PriceType']\n",
    "\n",
    "# Apply the functions, skipping NaN values\n",
    "tickets['Email'] = tickets['Email'].apply(lambda x: format_emails(x) if pd.notna(x) else x)\n",
    "tickets['Email'] = tickets['Email'].apply(lambda x: format_invalid_emails(x) if pd.notna(x) else x)\n",
    "\n",
    "# Drop rows where Email is unknown (NaN or empty string)\n",
    "tickets = tickets[tickets['Email'].notna() & (tickets['Email'] != '')]\n",
    "\n",
    "tickets.to_csv('Dont Import Tickets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Transform Tickets into Transcations for Import\n",
    "\n",
    "Now we have all our tickets cleaned up we are ready to group these into their specific transactions which will be the deals we will import\n",
    "\n",
    "1. <b> Define prioritization of tickets </b> -- We want to prioritize what ticket type will assume the primary name of the [TicketType] in the Deal. When we import our Deals into HubSpot we can only have one TicketType\n",
    "2. <b> Group into transactions </b> -- Grouping is done by [TransactionID] and [Event Date] so we can see the specific breakdown of games attended. After grouping the required HubSpot properties are added and we set the [Deal Owner] and [BusinessUnit] to the correct name. These Deals are then named and ready for import into HubSpot.\n",
    "3. <b> Add properties needed for import </b> -- Not all the properties we want to include or that are needed for an import into HubSpot are in the data so after we group into transactions we want to add these properties in\n",
    "\n",
    "All column names match those they are importing to but one area to keep a note on is when importing the [Close Date] property will default to be a contact property. <b> Close Date must be switched to the deal property under the same name or the deal closed dates will be set to the import date. </b>\n",
    "\n",
    "If you get an error saying Duplicate Association ID for the deals after import these deals were still imported just without an association and you will need to go into each deal and associate them manually. This is not an error that should typically come up in small imports.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = tickets.sort_values(by=['Email', 'TransactNum'], ascending=[True, True])\n",
    "\n",
    "# Define a function to prioritize TicketType\n",
    "def prioritize_ticket_type(ticket_types):\n",
    "    # Define the priority order\n",
    "    priority_order = ['Single', 'Group', 'Season', 'Season Comp', 'Comp']\n",
    "    \n",
    "    # Remove NaN values and sort based on the priority\n",
    "    ticket_types = [t for t in ticket_types if pd.notna(t)]\n",
    "    for priority in priority_order:\n",
    "        if priority in ticket_types:\n",
    "            return priority\n",
    "    return 'Other'  # Default if none of the types match\n",
    "\n",
    "deals = tickets.groupby(['TransactNum', 'EventDate', 'Email']).agg(\n",
    "    Closed_Date=('Close Date', 'first'),\n",
    "    Opponent=('Opponent', 'first'),\n",
    "    Section=('Section', 'first'),\n",
    "    line_item_name=('line_item_name', 'first'),\n",
    "    Individual_Deal_Type=('Individual Deal Type', prioritize_ticket_type),\n",
    "    Amount=('Amount', 'sum'),\n",
    "    Number_of_Tickets=('Amount', 'size'),\n",
    "    Number_of_Tickets_Scanned=('IsScanned', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Format 'Amount' to two decimal places\n",
    "deals['Amount'] = deals['Amount'].round(2)\n",
    "\n",
    "# Add the new columns\n",
    "deals['Pipeline'] = 'Individual Tickets Pipeline'   # Set Pipeline as 'Individual Tickets'\n",
    "deals['Deal Stage'] = 'Closed Won'                  # Set Deal Stage as 'Closed Won'\n",
    "deals['Deal type'] = 'Individual Ticket'            # Set Deal type as 'Individual Ticket\n",
    "deals['Season'] = Season                            # Set Season as the current season\n",
    "deals['Ticketing Platform'] = 'SeatGeek'            # Set the Ticketing Platform\n",
    "deals['Deal Owner'] = 'info@loudoununitedfc.com'    # Set Deal Owner\n",
    "deals['Business units'] = 'Loudoun United FC'       # Set the business unit\n",
    "\n",
    "# Rename columns in the Deals dataframe\n",
    "deals.rename(columns={\n",
    "    'TransactNum': 'SeatGeek transaction number',\n",
    "    'Individual_Deal_Type': 'Individual Ticket Type'\n",
    "}, inplace=True)\n",
    "\n",
    "# Name the deal\n",
    "deals['Deal Name'] = deals['Season'].astype(str) + ' ' + deals['Opponent'].astype(str) + ' ' + deals['Number_of_Tickets'].astype(str) + ' ' + deals['Individual Ticket Type'] + ' Tickets'\n",
    "\n",
    "deals.to_csv('Import Deals.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Export Deals from HubSpot and Merge with Line Items\n",
    "\n",
    "When updating deals in HubSpot you need the Record ID of the deal. Since line items will be added onto the deals we need to <b> export the deals we created along with their Record ID, SeatGeek transaction number, Associated Contact and Event Date </b> to merge the Record IDs for the Deals to the line items. \n",
    "\n",
    "In doing this we are using the <b> tickets </b> dataframe that we had saved previously after cleaning our data <br>\n",
    "Export the deals from HubSpot and save them as <b> export.csv </b> in the same location as the code\n",
    "\n",
    "1. <b> Match the Event Date, Transaction Number, Email </b> -- Since we are merging on [Event Date] we need to ensure that the formats of the event dates are the same across dataframes\n",
    "2. <b> Merge data </b> -- Here we merge the data to get the Record ID of deals associated with the line items. We do a left join because we know that for each deal there must be associating line items as deals all have at least one ticket associated with them and the line items are representative of the tickets. It is import to <b> check all line items have a matching deal </b> before importing\n",
    "3. <b> Create Line Item properties </b> -- There are 3 required properties for creating a line item in HubSpot. These 3 properties are Name, Quantity, and Price. Line item name was previously created and is something we are grouping by. We follow this up with creating the Price and Quantity properties. These take all the tickets with the same name from the transaction and counts the number of tickets and takes the average price in order to have the amount in our line items match the amount of the deal. <b> GROUP BY </b>\n",
    "\n",
    "Once these are merged and added we can export this file for import into HubSpot. HubSpot can't take line items with negative values so it is important to set line items with values less than zero to equal zero before importing (there should be none anyway)\n",
    "\n",
    "To import into HubSpot:\n",
    "1. Select Deals and Line Items\n",
    "2. Set to update Deals and create Line Items\n",
    "3. Select Record ID as a deal property matching to Record ID\n",
    "4. Set Name, Quantity, and Price to be line item properties under the same names\n",
    "5. This should result in no errors in the preview and can go ahead and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_deals = pd.read_csv('export.csv')\n",
    "\n",
    "# Extract the email part from the 'Associated Contact' column and create a new 'Email' column\n",
    "exported_deals['Email'] = exported_deals['Associated Contact'].str.extract(r'\\((.*?)\\)')\n",
    "\n",
    "# Convert all Email values in both dataframes to lowercase\n",
    "tickets['Email'] = tickets['Email'].str.lower()\n",
    "exported_deals['Email'] = exported_deals['Email'].str.lower()\n",
    "\n",
    "# Convert 'EventDate' in tickets to match the format of 'Event Date' in exported_deals\n",
    "tickets['EventDate'] = pd.to_datetime(tickets['EventDate'], errors='coerce').dt.date.astype(str)\n",
    "\n",
    "# Ensure 'Event Date' in exported_deals is also in string format (if needed)\n",
    "exported_deals['Event Date'] = pd.to_datetime(exported_deals['Event Date'], errors='coerce').dt.date.astype(str)\n",
    "\n",
    "# Ensure 'seatgeek transaction number' in exported_deals is an integer\n",
    "exported_deals['seatgeek transaction number'] = pd.to_numeric(exported_deals['seatgeek transaction number'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Ensure 'TransactNum' in tickets is an integer\n",
    "tickets['TransactNum'] = pd.to_numeric(tickets['TransactNum'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Merge the dataframes and bring over 'Record ID' from exported_deals into tickets\n",
    "tickets = tickets.merge(\n",
    "    exported_deals[['Record ID', 'Event Date', 'seatgeek transaction number', 'Email']],\n",
    "    left_on=['EventDate', 'TransactNum', 'Email'],\n",
    "    right_on=['Event Date', 'seatgeek transaction number', 'Email'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "tickets.to_csv('line items test.csv')\n",
    "\n",
    "# Group tickets by TransactNum, EventDate, Email, and line_item_name\n",
    "line_items = tickets.groupby(['TransactNum', 'EventDate', 'Email', 'line_item_name']).agg(\n",
    "    Quantity=('Amount', 'size'),  # Count of rows\n",
    "    Price=('Amount', 'mean'),  # Average of Amount\n",
    "    RecordID = ('Record ID', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Keep only the required columns\n",
    "line_items = line_items[['RecordID', 'line_item_name', 'Quantity', 'Price']]\n",
    "\n",
    "# Save the resulting dataframe\n",
    "line_items.to_csv('Import Line Items.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
